# Speak app application: Prompt evaluator
I've built a simple prompt evaluator to assess the effectiveness of the conversational prompts.

Given 2 prompts A and B, we keep track of each prompt and response generated by the user. We then use LLM as a judge to evaluate each pair of prompt and response, and judge the prompt based on the following qualities of the prompt:
1. Length of response (does this prompt encourage the user to speak more?)
2. Lexical richness (does this prompt encourage the user to use more vocabulary?)
3. Relevance of response to topic (is this prompt clear?)
